"""
Final IPFIX Data Simulator for the 'enriched_flows' table.
Generates realistic network flow data, including anomalies, and sends it to Kafka.
This version completely omits GeoIP and IPAM fields from the output.
"""

import ipaddress
import json
import random
import time
import uuid
import logging
import argparse
import yaml
import csv
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Any
import numpy as np
from faker import Faker
from kafka import KafkaProducer

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('enriched-ipfix-simulator')

# Initialize Faker
fake = Faker()

def random_mac():
    return ':'.join([f'{random.randint(0, 255):02x}' for _ in range(6)])

def load_ip_ranges(csv_file):
    ip_ranges = []
    try:
        with open(csv_file, 'r') as file:
            reader = csv.reader(file)
            for row in reader:
                ip_ranges.append((int(row[0]), int(row[1])))
    except FileNotFoundError:
        logger.warning(f"IP range file '{csv_file}' not found. Using private IP ranges as a fallback.")
        return [
            (int(ipaddress.IPv4Address('10.0.0.0')), int(ipaddress.IPv4Address('10.255.255.255'))),
            (int(ipaddress.IPv4Address('172.16.0.0')), int(ipaddress.IPv4Address('172.31.255.255'))),
            (int(ipaddress.IPv4Address('192.168.0.0')), int(ipaddress.IPv4Address('192.168.255.255'))),
        ]
    return ip_ranges

def generate_random_ip(ip_ranges):
    if not ip_ranges: return fake.ipv4()
    start_ip, end_ip = random.choice(ip_ranges)
    return str(ipaddress.IPv4Address(random.randint(start_ip, end_ip)))

ip_ranges = load_ip_ranges('geolite/IP2LOCATION-LITE-DB1.csv')

class TrafficProfile:
    """Defines a network traffic pattern for the new schema."""
    def __init__(self, config: Dict[str, Any]):
        self.name = config['name']
        self.source_networks = config['source_networks']
        self.destination_networks = config['destination_networks']
        self.protocols = config['protocols']
        self.port_ranges = config['port_ranges']
        self.flow_size_distribution = config.get('flow_size_distribution', {'type': 'lognormal', 'mean': 7.5, 'sigma': 1.5})
        self.packet_size_distribution = config.get('packet_size_distribution', {'type': 'normal', 'mean': 800, 'sigma': 400, 'min': 64, 'max': 1500})
        self.flow_duration_distribution = config.get('flow_duration_distribution', {'type': 'lognormal', 'mean': 8.0, 'sigma': 1.2})
        self.time_pattern = config.get('time_pattern', {'type': 'constant', 'flows_per_minute': 100})
        self.rst_probability = config.get('rst_probability', 0.02)
        self.fin_probability = config.get('fin_probability', 0.8)

    def generate_ip(self, networks: List[str]) -> str:
        """Generate a random IP from a list of CIDR networks."""
        network = ipaddress.ip_network(random.choice(networks))
        return str(ipaddress.ip_address(random.randint(int(network.network_address), int(network.broadcast_address))))

    def generate_flow(self, timestamp: int) -> Dict[str, Any]:
        """Generate a random flow based on this profile."""
        source_ip = self.generate_ip(self.source_networks)
        destination_ip = self.generate_ip(self.destination_networks)
        protocol = random.choice(self.protocols)

        source_port = random.randint(*self.port_ranges['source'])
        destination_port = random.choice(self.port_ranges['destination'])

        # Generate flow size and duration
        flow_size = int(np.random.lognormal(self.flow_size_distribution['mean'], self.flow_size_distribution['sigma']))
        avg_packet_size = int(np.random.normal(self.packet_size_distribution.get('mean', 800), self.packet_size_distribution.get('sigma', 300)))
        avg_packet_size = min(max(avg_packet_size, self.packet_size_distribution.get('min', 64)), self.packet_size_distribution.get('max', 1500))
        packet_count = max(1, int(flow_size / avg_packet_size)) if avg_packet_size > 0 else 1
        duration_ms = int(np.random.lognormal(self.flow_duration_distribution['mean'], self.flow_duration_distribution['sigma']))

        start_time = datetime.fromtimestamp(timestamp, tz=timezone.utc)
        end_time = start_time + timedelta(milliseconds=duration_ms)

        tcp_flags = 0  # Bitmask: FIN=1, RST=4
        if protocol == 6:
            if random.random() < self.rst_probability: tcp_flags |= 4
            elif random.random() < self.fin_probability: tcp_flags |= 1

        client_ip, client_port, server_ip, server_port = source_ip, source_port, destination_ip, destination_port
        
        now = datetime.now(timezone.utc)
        flow = {
            'flow_id': str(uuid.uuid4()),
            'flow_key': f"{source_ip}:{source_port}-{destination_ip}:{destination_port}-{protocol}",
            'customer_id': 'customer-123',
            'flow_format': 'IPFIX',
            'exporter_id': 'exporter-01',
            'observation_domain_id': '1',
            'template_id': str(random.randint(256, 512)),
            'source_ip': source_ip,
            'source_port': source_port,
            'destination_ip': destination_ip,
            'destination_port': destination_port,
            'protocol': protocol,
            'sampler_address': '192.168.1.1',
            'sequence_number': random.randint(1000, 999999),
            'client_ip': client_ip,
            'client_port': client_port,
            'server_ip': server_ip,
            'server_port': server_port,
            'flow_direction': 'CLIENT_TO_SERVER',
            'direction_confidence': 0.9,
            'direction_reason': 'PORT_BASED',
            'first_seen': start_time.isoformat(timespec='milliseconds').replace('+00:00', 'Z'),
            'last_seen': end_time.isoformat(timespec='milliseconds').replace('+00:00', 'Z'),
            'duration': duration_ms / 1000.0,
            'enrichment_timestamp': now.isoformat(timespec='milliseconds').replace('+00:00', 'Z'),
            'processing_latency': random.randint(5, 50),
            'total_packets': packet_count,
            'total_bytes': flow_size,
            'forward_packets': packet_count,
            'forward_bytes': flow_size,
            'reverse_packets': 0,
            'reverse_bytes': 0,
            'tcp_flags': tcp_flags,
            'tos': 0,
            'source_dns': '',
            'destination_dns': '',
            'application': 'unknown',
            'custom_fields': {'profile_name': self.name},
            'ingestion_time': now.isoformat(timespec='milliseconds').replace('+00:00', 'Z'),
            'data_version': 1
        }
        return flow

    def get_flow_rate(self, current_time: datetime) -> int:
        base_rate = self.time_pattern.get('flows_per_minute', 100)
        pattern_type = self.time_pattern.get('type', 'constant')
        hour, day_of_week = current_time.hour, current_time.weekday()
        
        if pattern_type == 'diurnal':
            hour_factor = 0.5 + 0.5 * np.exp(-0.5 * ((hour - 12) / 4) ** 2)
            return int(base_rate * hour_factor)
        elif pattern_type == 'weekly':
            day_factor = 0.5 if day_of_week >= 5 else 1.0
            hour_factor = 0.5 + 0.5 * np.exp(-0.5 * ((hour - 12) / 4) ** 2)
            return int(base_rate * day_factor * hour_factor)
        else: # constant
            return base_rate

class AnomalyGenerator:
    """Generates network traffic anomalies."""
    def __init__(self, config: Dict[str, Any]):
        self.anomalies = config.get('anomalies', [])

    def get_active_anomalies(self, current_time: datetime) -> List[Dict[str, Any]]:
        active = []
        for anomaly in self.anomalies:
            start_time = datetime.fromisoformat(anomaly['start_time']).replace(tzinfo=timezone.utc)
            end_time = datetime.fromisoformat(anomaly['end_time']).replace(tzinfo=timezone.utc)
            if start_time <= current_time <= end_time:
                active.append(anomaly)
        return active

    def apply_anomalies(self, flows: List[Dict[str, Any]], active_anomalies: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        if not active_anomalies:
            return flows

        modified_flows = flows.copy()
        now_utc = datetime.now(timezone.utc)
        
        for anomaly in active_anomalies:
            anomaly_type = anomaly['type']

            if anomaly_type == 'volumetric_ddos':
                for _ in range(anomaly['flow_count']):
                    source_ip = fake.ipv4()
                    target_ip = anomaly['target_ip']
                    protocol = random.choice([6, 17])
                    start_time = now_utc - timedelta(milliseconds=random.randint(10, 100))
                    
                    attack_flow = {
                        'flow_id': str(uuid.uuid4()), 'flow_key': f"{source_ip}:0-{target_ip}:0-{protocol}",
                        'customer_id': 'customer-123', 'flow_format': 'IPFIX', 'exporter_id': 'exporter-01',
                        'source_ip': source_ip, 'source_port': random.randint(1024, 65535),
                        'destination_ip': target_ip, 'destination_port': anomaly.get('target_port', 80),
                        'protocol': protocol, 'client_ip': source_ip, 'server_ip': target_ip,
                        'first_seen': start_time.isoformat(timespec='milliseconds').replace('+00:00', 'Z'),
                        'last_seen': now_utc.isoformat(timespec='milliseconds').replace('+00:00', 'Z'),
                        'duration': (now_utc - start_time).total_seconds(),
                        'total_packets': 1, 'total_bytes': random.randint(60, 1500),
                        'forward_packets': 1, 'forward_bytes': random.randint(60, 1500),
                        'reverse_packets': 0, 'reverse_bytes': 0, 'tcp_flags': 0, 'tos': 0,
                        'application': 'http',
                        'custom_fields': {'anomaly': 'True', 'anomaly_type': 'volumetric_ddos'},
                        'ingestion_time': now_utc.isoformat(timespec='milliseconds').replace('+00:00', 'Z'), 'data_version': 1
                    }
                    modified_flows.append(attack_flow)

            elif anomaly_type == 'port_scan':
                for i in range(anomaly['port_count']):
                    source_ip = anomaly['source_ip']
                    target_ip = anomaly['target_ip']
                    start_time = now_utc - timedelta(milliseconds=random.randint(5, 50))

                    scan_flow = {
                        'flow_id': str(uuid.uuid4()), 'flow_key': f"{source_ip}:0-{target_ip}:{i+1}-6",
                        'customer_id': 'customer-123', 'flow_format': 'IPFIX', 'exporter_id': 'exporter-01',
                        'source_ip': source_ip, 'source_port': random.randint(1024, 65535),
                        'destination_ip': target_ip, 'destination_port': i + 1,
                        'protocol': 6, 'client_ip': source_ip, 'server_ip': target_ip,
                        'first_seen': start_time.isoformat(timespec='milliseconds').replace('+00:00', 'Z'),
                        'last_seen': now_utc.isoformat(timespec='milliseconds').replace('+00:00', 'Z'),
                        'duration': (now_utc - start_time).total_seconds(),
                        'total_packets': 1, 'total_bytes': random.randint(40, 100),
                        'forward_packets': 1, 'forward_bytes': random.randint(40, 100),
                        'reverse_packets': 0, 'reverse_bytes': 0, 'tcp_flags': random.choice([0, 4]), 'tos': 0,
                        'application': 'unknown',
                        'custom_fields': {'anomaly': 'True', 'anomaly_type': 'port_scan'},
                        'ingestion_time': now_utc.isoformat(timespec='milliseconds').replace('+00:00', 'Z'), 'data_version': 1
                    }
                    modified_flows.append(scan_flow)
        return modified_flows

class KafkaFlowProducer:
    """Sends flow data to Kafka."""
    def __init__(self, bootstrap_servers: str, topic: str):
        self.bootstrap_servers = bootstrap_servers
        self.topic = topic
        self.producer = None
        self.connect()

    def connect(self):
        try:
            self.producer = KafkaProducer(
                bootstrap_servers=self.bootstrap_servers,
                value_serializer=lambda x: json.dumps(x).encode('utf-8')
            )
            logger.info(f"Connected to Kafka at {self.bootstrap_servers}")
        except Exception as e:
            logger.error(f"Failed to connect to Kafka: {e}")

    def send_flows(self, flows: List[Dict[str, Any]]):
        if not self.producer: return
        for flow in flows: self.producer.send(self.topic, flow)
        self.producer.flush()

    def close(self):
        if self.producer: self.producer.close()

class IPFIXSimulator:
    """Main simulator class."""
    def __init__(self, config_file: str):
        with open(config_file, 'r') as f: self.config = yaml.safe_load(f)
        
        kafka_config = self.config['kafka']
        self.kafka_producer = KafkaFlowProducer(kafka_config['bootstrap_servers'], kafka_config['topic'])
        self.profiles = [TrafficProfile(p) for p in self.config['traffic_profiles']]
        self.anomaly_generator = AnomalyGenerator(self.config.get('anomalies', {}))

        self.simulation_speed = self.config.get('simulation_speed', 1.0)
        self.start_time = datetime.fromisoformat(self.config['start_time']).replace(tzinfo=timezone.utc)
        self.end_time = datetime.fromisoformat(self.config['end_time']).replace(tzinfo=timezone.utc)
        self.current_time = self.start_time
        self.running = False

    def start(self):
        logger.info(f"Starting simulation from {self.start_time} to {self.end_time}")
        self.running = True
        self.run_simulation()

    def stop(self):
        self.running = False
        self.kafka_producer.close()
        logger.info("Simulation stopped")

    def run_simulation(self):
        while self.running and self.current_time <= self.end_time:
            all_flows = []
            for profile in self.profiles:
                flow_rate_per_min = profile.get_flow_rate(self.current_time)
                num_flows_this_second = int(flow_rate_per_min / 60 * self.simulation_speed)
                for _ in range(num_flows_this_second):
                    all_flows.append(profile.generate_flow(int(self.current_time.timestamp())))
            
            active_anomalies = self.anomaly_generator.get_active_anomalies(self.current_time)
            all_flows = self.anomaly_generator.apply_anomalies(all_flows, active_anomalies)

            if all_flows:
                self.kafka_producer.send_flows(all_flows)
                logger.info(f"Time: {self.current_time} | Sent {len(all_flows)} flows (Anomalies active: {len(active_anomalies)})")

            self.current_time += timedelta(seconds=1 * self.simulation_speed)
            time.sleep(1)

def main():
    parser = argparse.ArgumentParser(description='Final IPFIX Traffic Simulator')
    parser.add_argument('--config', '-c', required=True, help='Configuration file path (e.g., config_final.yml)')
    args = parser.parse_args()

    simulator = IPFIXSimulator(args.config)
    try:
        simulator.start()
    except KeyboardInterrupt:
        logger.info("Simulation interrupted")
    finally:
        simulator.stop()

if __name__ == "__main__":
    main()
